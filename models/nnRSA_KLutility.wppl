
var num_utterances = 3
var num_worlds = 3
var h_size = 10

var sampleArray = function(a){
  var i = randomInteger(a.length)
  return a[i]
}

var sumScalars = function(a){
  var acc = function(b,sum){
    return b.length>0?  acc(b.slice(1),ad.scalar.add(sum,b[0])) :  sum
  }
  return acc(a,0)
}

var nnparam = function(a, b) {
  return param([a, b], 0, 0.1);
};

var network = function(x, W, b) {
  var input = T.reshape(T.concat(x[0],x[1]),[x[0].length+x[1].length,1])
  var h = T.tanh(T.add(T.dot(W[0], input), b[0]));
  var output = T.add(T.dot(W[1], h), b[1]);
  return T.sumreduce(output)
};

var worlds = [Vector([1,0,0]),Vector([0,1,0]),Vector([0,0,1])]

var knowledge_states1 = 
  [[Vector([1,0,0])],
  [Vector([0,1,0])],
  [Vector([0,0,1])]]
var knowledge_states2 = 
  [[Vector([1,0,0]),Vector([0,1,0]),Vector([0,0,1])],
  [Vector([0,0,1])]]

var knowledgePrior = function(knowledge_states){
  //var i = randomInteger(knowledge_states.length)
  //return knowledge_states[i]
  return sampleArray(knowledge_states)
}


var utterancePrior = function() {
  var utterances = [Vector([1,0,0]),Vector([0,1,0]),Vector([0,0,1])]
  //var i = randomInteger(utterances.length)
  return sampleArray(utterances)
}

//knowledge_state is a set of worlds
var speakerUtility = function(listener,knowledge_state) {
  var num_possible_worlds = knowledge_state.length
  var log_probs = map(function(w){return listener.score([],w)},knowledge_state)
  var sum_logs = sumScalars(log_probs)
  //console.log(map(function(w){return listener.score([],w)},knowledge_state))
  return ad.scalar.add(sum_logs,-1*Math.log(num_possible_worlds))

}

var literalListener = cache(function(utterance,W,b,knowledge_states) {
  Enumerate(function(){
    var knowledge_state = knowledgePrior(knowledge_states)
    var world = sampleArray(knowledge_state)

    factor(network([world,utterance],W,b))

    return world
  })
})

var speaker = cache(function(knowledge_state,W,b,knowledge_states) {
Enumerate(function(){
  var utterance = utterancePrior()
  var L = literalListener(utterance,W,b,knowledge_states)

  factor(speakerUtility(L,knowledge_state))

  return utterance
})})

var listener = cache(function(utterance,W,b,knowledge_states) {
Enumerate(function(){
  var knowledge_state = knowledgePrior(knowledge_states)
  var S = speaker(knowledge_state,W,b,knowledge_states)

  factor(S.score([],utterance))

  return knowledge_state
})
})

var sampleMatrixGaussian = function(dims,mean,variance,guide_mean){
  var length = dims[0]*dims[1]
  var g = sample(diagCovGaussianERP,[Vector(repeat(length, constF(mean))),Vector(repeat(length, constF(variance)))],
    {guide: [diagCovGaussianERP, [T.reshape(guide_mean,[length,1]),Vector(repeat(length, constF(0.001)))]]})
  return T.reshape(g,dims)
}

var model = function() {


  var W0_var = nnparam(h_size,num_worlds+num_utterances)
  var W1_var = nnparam(1,h_size)
  var b0_var = nnparam(h_size,1)
  var b1_var = nnparam(1,1)

  var W0 = sampleMatrixGaussian([h_size,num_worlds+num_utterances],0,10,W0_var)
  var W1 = sampleMatrixGaussian([1,h_size],0,10,W1_var)
  var b0 = sampleMatrixGaussian([h_size,1],0,10,b0_var)
  var b1 = sampleMatrixGaussian([1,1],0,10,b1_var)


  var l1 = listener(Vector([0,0,1]),[W0,W1],[b0,b1],knowledge_states1)
  var l2 = listener(Vector([1,0,0]),[W0,W1],[b0,b1],knowledge_states1)
  var l3 = listener(Vector([0,0,1]),[W0,W1],[b0,b1],knowledge_states2)
  var l4 = listener(Vector([1,0,0]),[W0,W1],[b0,b1],knowledge_states2)

  var lit = literalListener(Vector([1,0,0]),[W0,W1],[b0,b1],knowledge_states1)

  console.log(lit.hist)
  factor(l3.score([], knowledge_states2[1])*10+
    l4.score([], knowledge_states2[0])*10+
    l1.score([], knowledge_states1[2])*10+
    l2.score([], knowledge_states1[0])*10)

  return W0;
};

// VI.
var params = Optimize(model, {steps: 2000, method: {gd: {stepSize: 0.01}}, estimator: {ELBO: {samples: 1}}});

// Optimized params.
console.log(JSON.stringify(params))